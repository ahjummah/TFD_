{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from Preprocessing import remove_punctuations\n",
    "from Preprocessing import remove_stopwords\n",
    "from sklearn.utils import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('Models/classifier.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "    \n",
    "with open('Models/vectorizer.pkl', 'rb') as g:\n",
    "    vectorizer = pickle.load(g)\n",
    "    \n",
    "with open('Models/lsa.pkl', 'rb') as h:\n",
    "    lsa = pickle.load(h)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d18a3b8da70c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@jessa you are beautiful the very best do not believe them when they say you're #ugly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremovePunctuations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jessa/Desktop/198/Thesis-Final-2/TFD_/Preprocessing/remove_stopwords.py\u001b[0m in \u001b[0;36mremove_words\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m'having'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'do'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'does'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'did'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'doing'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'an'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'but'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'if'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'or'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'because'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'as'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'until'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'while'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'of'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'at'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'by'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'for'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'with'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'about'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'against'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'between'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'into'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'through'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'during'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;34m'before'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'after'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'above'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'below'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'to'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'up'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'down'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'in'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'over'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'under'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;34m'again'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'further'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'then'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'once'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'here'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'there'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'when'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'where'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'why'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'how'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m'each'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'few'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'more'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'most'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'other'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'some'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'such'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nor'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'only'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'own'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'so'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'than'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jessa/Desktop/198/Thesis-Final-2/TFD_/Preprocessing/remove_stopwords.py\u001b[0m in \u001b[0;36mretrieve_stopwords\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     stop = ['we','our','ours','ourselves','he','him','his','himself', 'she', 'her', 'hers', 'herself', 'it',\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m'its'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'itself'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'they'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'them'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'their'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'theirs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'themselves'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'what'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'which'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'who'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'whom'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m'that'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'these'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'those'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'am'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'was'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'were'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'be'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'been'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'being'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'have'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'had'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'stopwords_list.txt'"
     ]
    }
   ],
   "source": [
    "# a = raw_input(\"Enter:\\n\")\n",
    "a = \"@jessa you are beautiful the very best do not believe them when they say you're #ugly\"\n",
    "a = remove_punctuations.removePunctuations(a)\n",
    "a = remove_stopwords.remove_words(a)\n",
    "print a\n",
    "X = vectorizer.transform([a]) \n",
    "tfidf = X.toarray()\n",
    "lsa_ = lsa.transform(X)\n",
    "final_representation = np.concatenate((tfidf,lsa_),axis=1)\n",
    "print \"output\",clf.predict(final_representation)\n",
    "\n",
    "label = []\n",
    "_input = raw_input(\"Do you feel like you are being bullied?[0/1]:\\n\")\n",
    "label.append(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = \" @jessa you stupid\"\n",
    "b = remove_punctuations.removePunctuations(b)\n",
    "b = remove_stopwords.remove_words(b)\n",
    "\n",
    "X1 = vectorizer.transform([b])\n",
    "tfidf = X1.toarray()\n",
    "lsa_ = lsa.transform(X1)\n",
    "final_representation = np.concatenate((tfidf,lsa_),axis=1)\n",
    "print \"output\",clf.predict(final_representation)\n",
    "_input = raw_input(\"Do you feel like you are being bullied?[0/1]:\\n\")\n",
    "label.append(_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_feed = []\n",
    "new_feed.append(a)\n",
    "new_feed.append(b)\n",
    "c = []\n",
    "\n",
    "for tweet in new_feed:\n",
    "    c.append(remove_stopwords.remove_words(remove_punctuations.removePunctuations(tweet)))\n",
    "    \n",
    "X_new = np.array(c)   \n",
    "y_new = np.array(label)\n",
    "print X_new.shape\n",
    "print y_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2 = vectorizer.transform(X_new)\n",
    "tfidf = X2.toarray()\n",
    "lsa_ = lsa.transform(X2)\n",
    "final_representation = np.concatenate((tfidf,lsa_),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', [0, 1], y_new)\n",
    "class_weight_dictionary = {1:class_weights[0], 1:class_weights[1]}\n",
    "clf.partial_fit(final_representation,y_new,classes=[0,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
